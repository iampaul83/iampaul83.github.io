---
file_name: "_posts/2016-12-02-mongodb.md"
layout: post
title:  "使用Mongodb"
date:   2016-12-2 14:34:48 +0800
categories: spark datasource
tags: datasource mongodb
author: iampaul83
sidebar:
  nav: "spark"
---

Spark使用Mongodb的方法，使用mongodb官方提供的connector



## 使用方式

開始之前先import

```scala
import com.mongodb.spark.sql._
```

### Read

```scala
val mongoDataFrame1 = spark.read
  .option("database", "IEDB")
  .option("collection", "EHAVE_TBL_2016")
  .mongo()
```

### Schema

如果沒有指定Schema的話，mongo driver會自動判斷。__但是有時候會壞掉！所以還是不要偷懶！__

```scala
// 用case class定義schema
case class EHAVE_TBL(
    CNAME: String, CCOVEY: String, ISCORE: String,
    IFPASS: String, FACTORY: String, ISEX: String
)

val mongoDataFrame3 = spark.read
    .option("database", "IEDB")
    .option("collection", "EHAVE_TBL_2016")
    // 指定schema
    .mongo[EHAVE_TBL]()
```

### Write

overwrite模式參考以下[文件](https://docs.mongodb.com/spark-connector/spark-sql/)的"Save DataFrames to MongoDB"

```scala
mongoDataFrame3.write
  .option("database", "spark_tmp")
  .option("collection", "passCountByFactorySex")
  // 確認是否要overwrite
  //.mode("overwrite")
  .mongo()
```

## DataFrameReader

[DataFrameReader]: https://spark.apache.org/docs/1.6.1/api/scala/index.html#org.apache.spark.sql.DataFrameReader
[DataFrameWriter]: https://spark.apache.org/docs/1.6.1/api/scala/index.html#org.apache.spark.sql.DataFrameWriter

其實上面範例中用的是[`spark.sql.DataFrameReader`][DataFrameReader]與[`DataFrameWriter`][DataFrameWriter]，在眾多連線語法中，我覺得這個最可靠（該有的設定都有，語法簡潔）

## DataFrameReader.mongo()

上面使用DataFrameReader建立DataFrame，`.mongo()`是`.format("com.mongodb.spark.sql").load()`的縮寫


## uri呢？

一般連mongo會有一個uri，例如
```r
mongodb://192.168.2.201:27017,192.168.2.203:27017,192.168.2.203:27017/?replicaSet=bigdata&readPreference=secondaryPreferred
```
用來指定mongod的位置，資料庫，或者其他選項，這邊沒有指定uri是因為把預設值寫在 __spark.mongodb.input.uri__ 與 __spark.mongodb.output.uri__ 了，詳情請看[Spark Interpreter Setting](http://192.168.2.230/service/zeppelin2/#/interpreter)
